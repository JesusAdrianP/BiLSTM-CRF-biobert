{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Este modelo de red entrena un BiLSTM + CRF para la clasificación de NER sobre el corpus Biobert. Este modelo tiene como entrada a la red la enterización del conjunto X de entrenamiento y la enterización y categorización  de los vectores de etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\\fasttextspanish\\text_fasttext_skip_model_300_test.txt\n",
      "inputs\\libs2021\\mwrapper.py\n",
      "inputs\\libs2021\\tf2crf.py\n",
      "inputs\\libs2021\\utils.py\n",
      "inputs\\libs2021\\__pycache__\\mwrapper.cpython-310.pyc\n",
      "inputs\\libs2021\\__pycache__\\tf2crf.cpython-310.pyc\n",
      "inputs\\libs2021\\__pycache__\\utils.cpython-310.pyc\n",
      "inputs\\libscrf4\\crfta.py\n",
      "inputs\\libscrf4\\utils.py\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('inputs'):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Univalle\\Downloads\\BiLSTM-CRF-biobert\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Univalle\\AppData\\Local\\Temp\\ipykernel_20072\\2502262557.py:61: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('inputs/libs2021')\n",
    "\n",
    "#sys.path.append('/inputs/embedding')\n",
    "\n",
    "sys.path.append('inputs/libscrf4')\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report as eskclarep\n",
    "\n",
    "#from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "#from seqeval.metrics import classification_report as seqclarep\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Lambda, Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, InputLayer, Activation, Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "\n",
    "#from crfta import CRF as crf4\n",
    "\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "import datetime, os\n",
    "\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga del dataset Biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "biobert_dataset = load_dataset('Biobert_json.py', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9788 2\n",
      "2496 2\n",
      "2758 2\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 502 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##biobert\n",
    "\n",
    "bio_train_sents = list(biobert_dataset['train'])\n",
    "\n",
    "bio_test_sents = list(biobert_dataset['test'])\n",
    "\n",
    "bio_eval_sents = list(biobert_dataset['validation'])\n",
    "\n",
    "print(len(bio_train_sents),len(max(bio_train_sents,key=len)))\n",
    "\n",
    "print(len(bio_test_sents),len(max(bio_test_sents,key=len)))\n",
    "\n",
    "print(len(bio_eval_sents),len(max(bio_eval_sents,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentencia': ['Abuela', 'materna', 'con', 'cancer', 'de', 'mama', 'a', 'los', '70', 'años', '.'], 'tag': [4, 19, 29, 0, 16, 16, 29, 29, 10, 8, 29]}\n"
     ]
    }
   ],
   "source": [
    "print(bio_train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9788"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biobert_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE  1. PREPROCESAMIENTO DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##biobert\n",
    "X_bio_train = [s['sentencia'] for s in bio_train_sents]\n",
    "\n",
    "y_bio_train = [s['tag'] for s in bio_train_sents]\n",
    "\n",
    "\n",
    "\n",
    "X_bio_test = [s['sentencia'] for s in bio_test_sents]\n",
    "\n",
    "y_bio_test = [s['tag'] for s in bio_test_sents]\n",
    "\n",
    "\n",
    "\n",
    "X_bio_eval = [s['sentencia'] for s in bio_eval_sents]\n",
    "\n",
    "y_bio_eval = [s['tag'] for s in bio_eval_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', 'Quiste', 'renal', 'izquierdo', 'complicado', '(', 'ecografia', 'noviembre', '2013', 'quistes', 'renales', 'bilaterales', ')', '.']\n",
      "[29, 29, 29, 29, 29, 29, 29, 2, 17, 29, 29, 29, 29, 29]\n"
     ]
    }
   ],
   "source": [
    "print(X_bio_train[2])\n",
    "\n",
    "print(y_bio_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9883\n",
      "32\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29}\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "bio_words, bio_tagsss = set([]), set([])\n",
    "\n",
    " \n",
    "\n",
    "for s in (X_bio_train + X_bio_eval + X_bio_test):\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        bio_words.add(w.lower())\n",
    "\n",
    "\n",
    "\n",
    "for ts in (y_bio_train + y_bio_eval + y_bio_test):\n",
    "\n",
    "    for t in ts:\n",
    "\n",
    "        bio_tagsss.add(t)\n",
    "\n",
    "\n",
    "\n",
    "bio_word2index = {w: i + 2 for i, w in enumerate(list(bio_words))}\n",
    "\n",
    "bio_word2index['-PAD-'] = 0  # The special value used for padding\n",
    "\n",
    "bio_word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    "\n",
    " \n",
    "\n",
    "bio_tag2index = {t: i + 2 for i, t in enumerate(list(bio_tagsss))}\n",
    "\n",
    "bio_tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "\n",
    "bio_tag2index['-OOV-'] = 1  # The special value used to padding\n",
    "\n",
    "\n",
    "\n",
    "print (len(bio_word2index))\n",
    "\n",
    "print (len(bio_tag2index))\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"outputs/bio_word2index.npy\", bio_word2index)\n",
    "\n",
    "np.save(\"outputs/bio_tag2index.npy\", bio_tag2index)\n",
    "\n",
    "print(bio_tagsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##biobert\n",
    "bio_train_sentences_X, bio_eval_sentences_X, bio_test_sentences_X, bio_train_tags_y, bio_eval_tags_y, bio_test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "for s in X_bio_train:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    " \n",
    "\n",
    "    bio_train_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in X_bio_eval:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    " \n",
    "\n",
    "    bio_eval_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in X_bio_test:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    " \n",
    "\n",
    "    bio_test_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in y_bio_train:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_tag2index['-OOV-'])\n",
    "\n",
    "            \n",
    "\n",
    "    bio_train_tags_y.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in y_bio_eval:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_tag2index['-OOV-'])\n",
    "\n",
    "            \n",
    "\n",
    "    bio_eval_tags_y.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in y_bio_test:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_tag2index['-OOV-'])\n",
    "\n",
    "            \n",
    "\n",
    "    bio_test_tags_y.append(s_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las matrices de los tags son de números indexados pequeños porque solo son 11 tags.  ({ORG, LOC, PER}  X IOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudes de las Matrices:\n",
      "9788\n",
      "2758\n",
      "2496\n",
      "9788\n",
      "2758\n",
      "2496\n",
      "\n",
      "Muestra de Datos presentes en las Matrices con las transformaciones:\n",
      "\n",
      "[9500, 8095, 7421, 6190, 6590, 2527, 6823, 6766, 5533, 8022, 3326]\n",
      "[8525, 6287, 9295, 1404, 2893, 7375, 6695, 7866, 9225, 5270, 5077, 2556, 8746, 5386, 9003, 3326]\n",
      "[5466, 6590, 3381, 8022, 7421, 6242, 8934, 6366, 8681, 1566, 3326]\n",
      "[6, 21, 31, 2, 18, 18, 31, 31, 12, 10, 31]\n",
      "[10, 12, 5, 10, 12, 31, 5, 12, 10, 31, 10, 31, 7, 22, 22, 31]\n",
      "[31, 31, 12, 10, 31, 31, 31, 31, 31, 4, 31]\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "print(\"Longitudes de las Matrices:\")\n",
    "\n",
    "print(len(bio_train_sentences_X))\n",
    "\n",
    "print(len(bio_eval_sentences_X))\n",
    "\n",
    "print(len(bio_test_sentences_X))\n",
    "\n",
    "print(len(bio_train_tags_y))\n",
    "\n",
    "print(len(bio_eval_tags_y))\n",
    "\n",
    "print(len(bio_test_tags_y))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMuestra de Datos presentes en las Matrices con las transformaciones:\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(bio_train_sentences_X[0])\n",
    "\n",
    "print(bio_eval_sentences_X[0])\n",
    "\n",
    "print(bio_test_sentences_X[0])\n",
    "\n",
    "print(bio_train_tags_y[0])\n",
    "\n",
    "print(bio_eval_tags_y[0])\n",
    "\n",
    "print(bio_test_tags_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se procede a Normalizar las matrices con la longitud de la columna=MAX_LENGTH1 para que todas contengan el mismo numero de columnas, con la longitud máxima de palabras encontradas anteriormente y se agregan ceros a la derecha en las posiciones que hacen falta en el vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9500 8095 7421 6190 6590 2527 6823 6766 5533 8022 3326    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(9788, 202)\n",
      "[8525 6287 9295 1404 2893 7375 6695 7866 9225 5270 5077 2556 8746 5386\n",
      " 9003 3326    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(2758, 202)\n",
      "[5466 6590 3381 8022 7421 6242 8934 6366 8681 1566 3326    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(2496, 202)\n",
      "[ 6 21 31  2 18 18 31 31 12 10 31  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "(9788, 202)\n",
      "[10 12  5 10 12 31  5 12 10 31 10 31  7 22 22 31  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "(2758, 202)\n",
      "[31 31 12 10 31 31 31 31 31  4 31  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "(2496, 202)\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "\n",
    "\n",
    "\n",
    "MAX_LENGTH=202 \n",
    "\n",
    "bio_train_sentences_X = pad_sequences(bio_train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_eval_sentences_X = pad_sequences(bio_eval_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_test_sentences_X = pad_sequences(bio_test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_train_tags_y = pad_sequences(bio_train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_eval_tags_y = pad_sequences(bio_eval_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_test_tags_y = pad_sequences(bio_test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    " \n",
    "\n",
    "print(bio_train_sentences_X[0])\n",
    "\n",
    "print(bio_train_sentences_X.shape)\n",
    "\n",
    "print(bio_eval_sentences_X[0])\n",
    "\n",
    "print(bio_eval_sentences_X.shape)\n",
    "\n",
    "print(bio_test_sentences_X[0])\n",
    "\n",
    "print(bio_test_sentences_X.shape)\n",
    "\n",
    "print(bio_train_tags_y[0])\n",
    "\n",
    "print(bio_train_tags_y.shape)\n",
    "\n",
    "print(bio_eval_tags_y[0])\n",
    "\n",
    "print(bio_eval_tags_y.shape)\n",
    "\n",
    "print(bio_test_tags_y[0])\n",
    "\n",
    "print(bio_test_tags_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"outputs/bio_train_sentences_X.npy\", bio_train_sentences_X)\n",
    "\n",
    "np.save(\"outputs/bio_eval_sentences_X.npy\", bio_eval_sentences_X)\n",
    "\n",
    "np.save(\"outputs/bio_test_sentences_X.npy\", bio_test_sentences_X)\n",
    "\n",
    "np.save(\"outputs/bio_train_tags_y.npy\", bio_train_tags_y)\n",
    "\n",
    "np.save(\"outputs/bio_eval_tags_y.npy\", bio_eval_tags_y)\n",
    "\n",
    "np.save(\"outputs/bio_test_tags_y.npy\", bio_test_tags_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categoricals(sequences, categories):\n",
    "\n",
    "    cat_sequences = []\n",
    "\n",
    "    for s in sequences:\n",
    "\n",
    "        cats = []\n",
    "\n",
    "        for item in s:\n",
    "\n",
    "            cats.append(np.zeros(categories))\n",
    "\n",
    "            cats[-1][item] = 1.0\n",
    "\n",
    "        cat_sequences.append(cats)\n",
    "\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data):\n",
    "\n",
    "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
    "\n",
    "    encoded = to_categorical(data)\n",
    "\n",
    "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se realiza la categorización one-hot de las etiquetas o labels de entrenamiento, testeo y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "9788\n",
      "2496\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "\n",
    "bio_cat_train_tags_y = to_categoricals(bio_train_tags_y, len(bio_tag2index))\n",
    "\n",
    "bio_cat_eval_tags_y  = to_categoricals(bio_eval_tags_y, len(bio_tag2index))\n",
    "\n",
    "bio_cat_test_tags_y  = to_categoricals(bio_test_tags_y, len(bio_tag2index))\n",
    "\n",
    "\n",
    "\n",
    "print(bio_cat_train_tags_y[1])\n",
    "\n",
    "print(len(bio_cat_train_tags_y))\n",
    "\n",
    "print(len(bio_cat_test_tags_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2. ENTRENAMIENTO DEL MODELO DE RED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713b6f8e44c54cfaaffaf568132afe46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrado 2000000 Word Vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eb70b2710a42df9a60aca146867d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertidos: 25347 Tokens | Perdidos: 3445 Tokens\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM=300\n",
    "\n",
    "file = 'inputs/fasttextspanish/text_fasttext_skip_model_300.txt'\n",
    "\n",
    "embedding_matrix = bme(file, len(bio_word2index), EMBED_DIM, bio_word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##biobert\n",
    "from tf2crf import CRF as crf6\n",
    "\n",
    "from mwrapper import ModelWithCRFLoss, ModelWithCRFLossDSCLoss\n",
    "\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Lambda, Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, InputLayer, Activation, Flatten, Masking\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "\n",
    "input = Input(shape=(MAX_LENGTH,))\n",
    "\n",
    "word_embedding_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Embedding Layer\n",
    "\n",
    "#model = Embedding(input_dim=len(word2index), \n",
    "\n",
    "    #            output_dim=word_embedding_size, \n",
    "\n",
    "     #           input_length=MAX_LENGTH,\n",
    "\n",
    "     #           mask_zero=False)(input)\n",
    "\n",
    "\n",
    "\n",
    "model = Embedding(len(bio_word2index),\n",
    "\n",
    "                        EMBED_DIM,\n",
    "\n",
    "                        input_length=MAX_LENGTH,  \n",
    "\n",
    "                        weights=[embedding_matrix],\n",
    "\n",
    "                        trainable=False,\n",
    "\n",
    "                        mask_zero=True)(input)\n",
    "\n",
    "\n",
    "\n",
    "# BI-LSTM Layer\n",
    "\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "\n",
    "                     return_sequences=True, \n",
    "\n",
    "                     dropout=0.5, \n",
    "\n",
    "                     recurrent_dropout=0.5))(model)\n",
    "\n",
    "model  = Dropout(0.5, name='dropout_lstm')(model)\n",
    "\n",
    "model  = Dense(units=EMBED_DIM * 2, activation='relu')(model)\n",
    "\n",
    "model  = Dense(units=len(bio_tag2index), activation='relu')(model)\n",
    "\n",
    "    \n",
    "\n",
    "model  = Masking(mask_value=0.,input_shape=(MAX_LENGTH, len(bio_tag2index)))(model)\n",
    "\n",
    "    \n",
    "\n",
    "crf = crf6(units=len(bio_tag2index), name=\"ner_crf\")\n",
    "\n",
    "predictions = crf(model)\n",
    "\n",
    "\n",
    "\n",
    "base_model = Model(inputs=input, outputs=predictions)\n",
    "\n",
    "model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
    "\n",
    "    \n",
    "\n",
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "153/153 - 592s - loss: 20.0810 - accuracy: 0.9783 - val_loss_val: 86.3857 - val_val_accuracy: 0.9217 - 592s/epoch - 4s/step\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "#biobert\n",
    "history= model.fit(bio_train_sentences_X, bio_cat_train_tags_y,\n",
    "\n",
    "                       validation_data=(bio_eval_sentences_X, bio_cat_eval_tags_y),\n",
    "\n",
    "                       batch_size=64, \n",
    "\n",
    "                       epochs=20,\n",
    "\n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 2, 'B-MISC': 3, 'I-ORG': 4, 'B-LOC': 5, 'B-ORG': 6, 'I-PER': 7, 'B-PER': 8, 'O': 9, 'I-MISC': 10, '-PAD-': 0, '-OOV-': 1}\n",
      "48/48 [==============================] - 37s 769ms/step\n",
      "(1517, 202)\n"
     ]
    }
   ],
   "source": [
    "print(bio_tag2index)\n",
    "\n",
    "y_pred= model.predict(bio_test_sentences_X)\n",
    "\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'I-LOC', 3: 'B-MISC', 4: 'I-ORG', 5: 'B-LOC', 6: 'B-ORG', 7: 'I-PER', 8: 'B-PER', 9: 'O', 10: 'I-MISC', 0: '-PAD-', 1: '-OOV-'}\n",
      "['O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "index2tag = {i: t for t, i in bio_tag2index.items()}\n",
    "\n",
    "print(index2tag)\n",
    "\n",
    "y1_pred = logits_to_tokens(y_pred, index2tag)\n",
    "\n",
    "print(y1_pred[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1517, 202)\n"
     ]
    }
   ],
   "source": [
    "#print(Y_test[4])\n",
    "\n",
    "print(bio_test_tags_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tags = biobert_dataset['test'].features['tag'].feature.names\n",
    "print(len(name_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_tagname(indexs, tags):\n",
    "    return [tags[i] if isinstance(i, int) else i for i in indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y1_pred = [index_to_tagname(i, name_tags) for i in y1_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_y1_pred[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'I-LOC', 3: 'B-MISC', 4: 'I-ORG', 5: 'B-LOC', 6: 'B-ORG', 7: 'I-PER', 8: 'B-PER', 9: 'O', 10: 'I-MISC', 0: '-PAD-', 1: '-OOV-'}\n",
      "['O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "index2tag = {i: t for t, i in bio_tag2index.items()}\n",
    "\n",
    "print(index2tag)\n",
    "\n",
    "y1_true = logits_to_tokens(bio_test_tags_y, index2tag)\n",
    "\n",
    "print(y1_true[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y1_true= [index_to_tagname(i, name_tags) for i in y1_true]\n",
    "\n",
    "print(new_y1_true[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 75.3%\n",
      "   recall: 28.0%\n",
      " accuracy: 98.4%\n",
      " F1-score: 40.9%\n"
     ]
    }
   ],
   "source": [
    "#hh1 = seqclarep(results['Expected'], results['Predicted'])\n",
    "\n",
    "#print('\\nclassification_report:\\n', hh1)\n",
    "\n",
    "from seqeval.metrics import classification_report as seqclarep\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(\"precision: {:.1%}\".format(precision_score(new_y1_true, new_y1_pred)))\n",
    "\n",
    "print(\"   recall: {:.1%}\".format(recall_score(new_y1_true, new_y1_pred)))\n",
    "\n",
    "print(\" accuracy: {:.1%}\".format(accuracy_score(new_y1_true, new_y1_pred)))\n",
    "\n",
    "print(\" F1-score: {:.1%}\".format(f1_score(new_y1_true, new_y1_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.588026Z",
     "iopub.status.idle": "2024-11-19T00:32:33.588657Z",
     "shell.execute_reply": "2024-11-19T00:32:33.588487Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.588466Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "li1 = sum(new_y1_true, [])\n",
    "\n",
    "li2 = sum(new_y1_pred, [])\n",
    "\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['Expected', 'Predicted'])\n",
    "\n",
    "\n",
    "\n",
    "results['Expected'] = li1\n",
    "\n",
    "results['Predicted'] = li2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.590511Z",
     "iopub.status.idle": "2024-11-19T00:32:33.590865Z",
     "shell.execute_reply": "2024-11-19T00:32:33.590710Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.590693Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report as eskclarep\n",
    "\n",
    "report = eskclarep(results['Expected'], results['Predicted'])\n",
    "\n",
    "#print('\\nclassification_report:\\n', report)\n",
    "\n",
    "\n",
    "\n",
    "print(report_to_df(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.592610Z",
     "iopub.status.idle": "2024-11-19T00:32:33.593002Z",
     "shell.execute_reply": "2024-11-19T00:32:33.592838Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.592818Z"
    }
   },
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "\n",
    "    \"James Rodriguez es el jugador colombiano más importante con Radamel Falcao.\".split(),\n",
    "\n",
    "    \" Jugadores de la selección Colombia que juegan en el Reino Unido\".split()\n",
    "\n",
    "]\n",
    "\n",
    "#print(max(test_samples))\n",
    "\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.594672Z",
     "iopub.status.idle": "2024-11-19T00:32:33.595082Z",
     "shell.execute_reply": "2024-11-19T00:32:33.594900Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.594880Z"
    }
   },
   "outputs": [],
   "source": [
    "test_samples_X = []\n",
    "\n",
    "for s in test_samples:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    "    test_samples_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(test_samples_X)\n",
    "\n",
    "print(test_samples_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.597357Z",
     "iopub.status.idle": "2024-11-19T00:32:33.598011Z",
     "shell.execute_reply": "2024-11-19T00:32:33.597635Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.597613Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.600028Z",
     "iopub.status.idle": "2024-11-19T00:32:33.600703Z",
     "shell.execute_reply": "2024-11-19T00:32:33.600419Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.600385Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(len(predictions))\n",
    "\n",
    "log_tokens = logits_to_tokens(predictions, {i: t for t, i in bio_tag2index.items()})\n",
    "\n",
    "print(log_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.602493Z",
     "iopub.status.idle": "2024-11-19T00:32:33.603151Z",
     "shell.execute_reply": "2024-11-19T00:32:33.602830Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.602800Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install tabulate\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "\n",
    "heads1 = test_samples[0]\n",
    "\n",
    "body1 = [log_tokens[0][:len(test_samples[0])]]\n",
    "\n",
    "\n",
    "\n",
    "heads2 = test_samples[1]\n",
    "\n",
    "body2 = [log_tokens[1][:len(test_samples[1])]]\n",
    "\n",
    "\n",
    "\n",
    "print(tabulate(body1, headers=heads1))\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(tabulate(body2, headers=heads2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## postagging Freeling 4.1\n",
    "\n",
    "\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con  bajo  índice   de  adrenalina  .\n",
    "\n",
    "## DA0MS0  NCMS000  AQ0MS00  VMIP3S0  SP    DA0MS0  NCMS000  SP   SP    NCMS000  SP  NCFS000     Fp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## pos tagger Stanford NLP\n",
    "\n",
    "\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con    bajo   índice  de    adrenalina  .\n",
    "\n",
    "## da0000  nc0s000  aq0000   vmip000  sp000 da0000  nc0s000  sp000  aq0000 nc0s000 sp000 nc0s000     fp"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1529457,
     "sourceId": 2524269,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1530256,
     "sourceId": 2525700,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1937937,
     "sourceId": 6353546,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
