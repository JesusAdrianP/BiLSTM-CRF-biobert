{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Este modelo de red entrena un BiLSTM + CRF para la clasificación de NER sobre el corpus Conll2002. Este modelo tiene como entrada a la red la enterización del conjunto X de entrenamiento y la enterización y categorización  de los vectores de etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T00:56:51.681951Z",
     "iopub.status.busy": "2024-11-19T00:56:51.681541Z",
     "iopub.status.idle": "2024-11-19T00:56:51.697285Z",
     "shell.execute_reply": "2024-11-19T00:56:51.695937Z",
     "shell.execute_reply.started": "2024-11-19T00:56:51.681915Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    import seqeval\n",
    "\n",
    "except ModuleNotFoundError as err:\n",
    "\n",
    "    !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\\ccfasttext\\cc.es.300.vec\n",
      "inputs\\ccfasttext\\word2vec_skip-gram_model_300.txt\n",
      "inputs\\libs2021\\mwrapper.py\n",
      "inputs\\libs2021\\tf2crf.py\n",
      "inputs\\libs2021\\utils.py\n",
      "inputs\\libs2021\\__pycache__\\mwrapper.cpython-310.pyc\n",
      "inputs\\libs2021\\__pycache__\\tf2crf.cpython-310.pyc\n",
      "inputs\\libs2021\\__pycache__\\utils.cpython-310.pyc\n",
      "inputs\\libscrf4\\crfta.py\n",
      "inputs\\libscrf4\\utils.py\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('inputs'):\n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Univalle\\AppData\\Local\\Temp\\ipykernel_13188\\2502262557.py:61: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('inputs/libs2021')\n",
    "\n",
    "#sys.path.append('/inputs/embedding')\n",
    "\n",
    "sys.path.append('inputs/libscrf4')\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report as eskclarep\n",
    "\n",
    "#from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "#from seqeval.metrics import classification_report as seqclarep\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Lambda, Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, InputLayer, Activation, Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "\n",
    "#from crfta import CRF as crf4\n",
    "\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "import datetime, os\n",
    "\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación del paquete nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\Univalle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('conll2002')\n",
    "\n",
    "nltk.corpus.conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9788 2\n",
      "2496 2\n",
      "2758 2\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 404 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##biobert\n",
    "\n",
    "bio_train_sents = list(biobert_dataset['train'])\n",
    "\n",
    "bio_test_sents = list(biobert_dataset['test'])\n",
    "\n",
    "bio_eval_sents = list(biobert_dataset['validation'])\n",
    "\n",
    "print(len(bio_train_sents),len(max(bio_train_sents,key=len)))\n",
    "\n",
    "print(len(bio_test_sents),len(max(bio_test_sents,key=len)))\n",
    "\n",
    "print(len(bio_eval_sents),len(max(bio_eval_sents,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentencia': ['Abuela', 'materna', 'con', 'cancer', 'de', 'mama', 'a', 'los', '70', 'años', '.'], 'tag': [4, 19, 29, 0, 16, 16, 29, 29, 10, 8, 29]}\n"
     ]
    }
   ],
   "source": [
    "print(bio_train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "     ---------------------------------------- 0.0/480.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 480.6/480.6 kB 14.7 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "     ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 179.3/179.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-18.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "     ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 1.9/25.1 MB 57.6 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 5.3/25.1 MB 67.1 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 9.2/25.1 MB 83.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 13.3/25.1 MB 93.0 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 16.9/25.1 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 21.8/25.1 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  25.1/25.1 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  25.1/25.1 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 25.1/25.1 MB 54.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Collecting huggingface-hub>=0.23.0\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "     ---------------------------------------- 0.0/447.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 447.5/447.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 116.3/116.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.6-cp310-cp310-win_amd64.whl (440 kB)\n",
      "     ---------------------------------------- 0.0/440.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 440.4/440.4 kB 26.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 134.8/134.8 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.17.2-cp310-cp310-win_amd64.whl (89 kB)\n",
      "     ---------------------------------------- 0.0/90.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.0/90.0 kB ? eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\univalle\\downloads\\punto2\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, filelock, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.11.6 aiosignal-1.3.1 async-timeout-5.0.1 datasets-3.1.0 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.26.2 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-18.0.0 xxhash-3.5.0 yarl-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190ec55557d846d0a445fd11ce7b206d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525d1375552a449fb8be7f4ea7208558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6679d376a34450d89f2ae181ae591d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "biobert_dataset = load_dataset('Biobert_json.py', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9788"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biobert_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE  1. PREPROCESAMIENTO DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Melbourne'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2tokens(train_sents[0])[0]\n",
    "\n",
    "#sent2labels(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##biobert\n",
    "X_bio_train = [s['sentencia'] for s in bio_train_sents]\n",
    "\n",
    "y_bio_train = [s['tag'] for s in bio_train_sents]\n",
    "\n",
    "\n",
    "\n",
    "X_bio_test = [s['sentencia'] for s in bio_test_sents]\n",
    "\n",
    "y_bio_test = [s['tag'] for s in bio_test_sents]\n",
    "\n",
    "\n",
    "\n",
    "X_bio_eval = [s['sentencia'] for s in bio_eval_sents]\n",
    "\n",
    "y_bio_eval = [s['tag'] for s in bio_eval_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', 'Quiste', 'renal', 'izquierdo', 'complicado', '(', 'ecografia', 'noviembre', '2013', 'quistes', 'renales', 'bilaterales', ')', '.']\n",
      "[29, 29, 29, 29, 29, 29, 29, 2, 17, 29, 29, 29, 29, 29]\n"
     ]
    }
   ],
   "source": [
    "print(X_bio_train[2])\n",
    "\n",
    "print(y_bio_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9883\n",
      "32\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29}\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "bio_words, bio_tagsss = set([]), set([])\n",
    "\n",
    " \n",
    "\n",
    "for s in (X_bio_train + X_bio_eval + X_bio_test):\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        bio_words.add(w.lower())\n",
    "\n",
    "\n",
    "\n",
    "for ts in (y_bio_train + y_bio_eval + y_bio_test):\n",
    "\n",
    "    for t in ts:\n",
    "\n",
    "        bio_tagsss.add(t)\n",
    "\n",
    "\n",
    "\n",
    "bio_word2index = {w: i + 2 for i, w in enumerate(list(bio_words))}\n",
    "\n",
    "bio_word2index['-PAD-'] = 0  # The special value used for padding\n",
    "\n",
    "bio_word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    "\n",
    " \n",
    "\n",
    "bio_tag2index = {t: i + 2 for i, t in enumerate(list(bio_tagsss))}\n",
    "\n",
    "bio_tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "\n",
    "bio_tag2index['-OOV-'] = 1  # The special value used to padding\n",
    "\n",
    "\n",
    "\n",
    "print (len(bio_word2index))\n",
    "\n",
    "print (len(bio_tag2index))\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"outputs/bio_word2index.npy\", bio_word2index)\n",
    "\n",
    "np.save(\"outputs/bio_tag2index.npy\", bio_tag2index)\n",
    "\n",
    "print(bio_tagsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##biobert\n",
    "bio_train_sentences_X, bio_eval_sentences_X, bio_test_sentences_X, bio_train_tags_y, bio_eval_tags_y, bio_test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "for s in X_bio_train:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    " \n",
    "\n",
    "    bio_train_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in X_bio_eval:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    " \n",
    "\n",
    "    bio_eval_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in X_bio_test:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_word2index['-OOV-'])\n",
    "\n",
    " \n",
    "\n",
    "    bio_test_sentences_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in y_bio_train:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_tag2index['-OOV-'])\n",
    "\n",
    "            \n",
    "\n",
    "    bio_train_tags_y.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in y_bio_eval:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_tag2index['-OOV-'])\n",
    "\n",
    "            \n",
    "\n",
    "    bio_eval_tags_y.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "for s in y_bio_test:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(bio_tag2index[w])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(bio_tag2index['-OOV-'])\n",
    "\n",
    "            \n",
    "\n",
    "    bio_test_tags_y.append(s_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las matrices de los tags son de números indexados pequeños porque solo son 11 tags.  ({ORG, LOC, PER}  X IOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudes de las Matrices:\n",
      "9788\n",
      "2758\n",
      "2496\n",
      "9788\n",
      "2758\n",
      "2496\n",
      "\n",
      "Muestra de Datos presentes en las Matrices con las transformaciones:\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[5160, 2455, 4788, 3023, 4902, 5502, 926, 7482, 656, 4339, 7292, 2442, 3830, 7360, 159, 9527]\n",
      "[8394, 3440, 7923, 5968, 6789, 9259, 590, 4275, 8491, 6019, 9527]\n",
      "[6, 21, 31, 2, 18, 18, 31, 31, 12, 10, 31]\n",
      "[10, 12, 5, 10, 12, 31, 5, 12, 10, 31, 10, 31, 7, 22, 22, 31]\n",
      "[31, 31, 12, 10, 31, 31, 31, 31, 31, 4, 31]\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "print(\"Longitudes de las Matrices:\")\n",
    "\n",
    "print(len(bio_train_sentences_X))\n",
    "\n",
    "print(len(bio_eval_sentences_X))\n",
    "\n",
    "print(len(bio_test_sentences_X))\n",
    "\n",
    "print(len(bio_train_tags_y))\n",
    "\n",
    "print(len(bio_eval_tags_y))\n",
    "\n",
    "print(len(bio_test_tags_y))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMuestra de Datos presentes en las Matrices con las transformaciones:\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(bio_train_sentences_X[0])\n",
    "\n",
    "print(bio_eval_sentences_X[0])\n",
    "\n",
    "print(bio_test_sentences_X[0])\n",
    "\n",
    "print(bio_train_tags_y[0])\n",
    "\n",
    "print(bio_eval_tags_y[0])\n",
    "\n",
    "print(bio_test_tags_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se procede a Normalizar las matrices con la longitud de la columna=MAX_LENGTH1 para que todas contengan el mismo numero de columnas, con la longitud máxima de palabras encontradas anteriormente y se agregan ceros a la derecha en las posiciones que hacen falta en el vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(9788, 202)\n",
      "[5160 2455 4788 3023 4902 5502  926 7482  656 4339 7292 2442 3830 7360\n",
      "  159 9527    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(2758, 202)\n",
      "[8394 3440 7923 5968 6789 9259  590 4275 8491 6019 9527    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(2496, 202)\n",
      "[ 6 21 31  2 18 18 31 31 12 10 31  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "(9788, 202)\n",
      "[10 12  5 10 12 31  5 12 10 31 10 31  7 22 22 31  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "(2758, 202)\n",
      "[31 31 12 10 31 31 31 31 31  4 31  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "(2496, 202)\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "\n",
    "\n",
    "\n",
    "MAX_LENGTH=202 \n",
    "\n",
    "bio_train_sentences_X = pad_sequences(bio_train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_eval_sentences_X = pad_sequences(bio_eval_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_test_sentences_X = pad_sequences(bio_test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_train_tags_y = pad_sequences(bio_train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_eval_tags_y = pad_sequences(bio_eval_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "bio_test_tags_y = pad_sequences(bio_test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    " \n",
    "\n",
    "print(bio_train_sentences_X[0])\n",
    "\n",
    "print(bio_train_sentences_X.shape)\n",
    "\n",
    "print(bio_eval_sentences_X[0])\n",
    "\n",
    "print(bio_eval_sentences_X.shape)\n",
    "\n",
    "print(bio_test_sentences_X[0])\n",
    "\n",
    "print(bio_test_sentences_X.shape)\n",
    "\n",
    "print(bio_train_tags_y[0])\n",
    "\n",
    "print(bio_train_tags_y.shape)\n",
    "\n",
    "print(bio_eval_tags_y[0])\n",
    "\n",
    "print(bio_eval_tags_y.shape)\n",
    "\n",
    "print(bio_test_tags_y[0])\n",
    "\n",
    "print(bio_test_tags_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"outputs/bio_train_sentences_X.npy\", bio_train_sentences_X)\n",
    "\n",
    "np.save(\"outputs/bio_eval_sentences_X.npy\", bio_eval_sentences_X)\n",
    "\n",
    "np.save(\"outputs/bio_test_sentences_X.npy\", bio_test_sentences_X)\n",
    "\n",
    "np.save(\"outputs/bio_train_tags_y.npy\", bio_train_tags_y)\n",
    "\n",
    "np.save(\"outputs/bio_eval_tags_y.npy\", bio_eval_tags_y)\n",
    "\n",
    "np.save(\"outputs/bio_test_tags_y.npy\", bio_test_tags_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categoricals(sequences, categories):\n",
    "\n",
    "    cat_sequences = []\n",
    "\n",
    "    for s in sequences:\n",
    "\n",
    "        cats = []\n",
    "\n",
    "        for item in s:\n",
    "\n",
    "            cats.append(np.zeros(categories))\n",
    "\n",
    "            cats[-1][item] = 1.0\n",
    "\n",
    "        cat_sequences.append(cats)\n",
    "\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode(data):\n",
    "\n",
    "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
    "\n",
    "    encoded = to_categorical(data)\n",
    "\n",
    "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se realiza la categorización one-hot de las etiquetas o labels de entrenamiento, testeo y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "9788\n",
      "2496\n"
     ]
    }
   ],
   "source": [
    "##biobert\n",
    "\n",
    "bio_cat_train_tags_y = to_categoricals(bio_train_tags_y, len(bio_tag2index))\n",
    "\n",
    "bio_cat_eval_tags_y  = to_categoricals(bio_eval_tags_y, len(bio_tag2index))\n",
    "\n",
    "bio_cat_test_tags_y  = to_categoricals(bio_test_tags_y, len(bio_tag2index))\n",
    "\n",
    "\n",
    "\n",
    "print(bio_cat_train_tags_y[1])\n",
    "\n",
    "print(len(bio_cat_train_tags_y))\n",
    "\n",
    "print(len(bio_cat_test_tags_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2. ENTRENAMIENTO DEL MODELO DE RED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713b6f8e44c54cfaaffaf568132afe46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrado 2000000 Word Vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eb70b2710a42df9a60aca146867d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertidos: 25347 Tokens | Perdidos: 3445 Tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "EMBED_DIM=300\n",
    "\n",
    "file = 'inputs/ccfasttext/cc.es.300.vec'\n",
    "\n",
    "embedding_matrix = bme(file, len(word2index), EMBED_DIM, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.1\n",
      "Keras version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "##biobert\n",
    "from tf2crf import CRF as crf6\n",
    "\n",
    "from mwrapper import ModelWithCRFLoss, ModelWithCRFLossDSCLoss\n",
    "\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Lambda, Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, InputLayer, Activation, Flatten, Masking\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "\n",
    "input = Input(shape=(MAX_LENGTH,))\n",
    "\n",
    "word_embedding_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Embedding Layer\n",
    "\n",
    "#model = Embedding(input_dim=len(word2index), \n",
    "\n",
    "    #            output_dim=word_embedding_size, \n",
    "\n",
    "     #           input_length=MAX_LENGTH,\n",
    "\n",
    "     #           mask_zero=False)(input)\n",
    "\n",
    "\n",
    "\n",
    "model = Embedding(len(bio_word2index),\n",
    "\n",
    "                        EMBED_DIM,\n",
    "\n",
    "                        input_length=MAX_LENGTH,  \n",
    "\n",
    "                        #weights=[embedding_matrix],\n",
    "\n",
    "                        trainable=False,\n",
    "\n",
    "                        mask_zero=True)(input)\n",
    "\n",
    "\n",
    "\n",
    "# BI-LSTM Layer\n",
    "\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "\n",
    "                     return_sequences=True, \n",
    "\n",
    "                     dropout=0.5, \n",
    "\n",
    "                     recurrent_dropout=0.5))(model)\n",
    "\n",
    "model  = Dropout(0.5, name='dropout_lstm')(model)\n",
    "\n",
    "model  = Dense(units=EMBED_DIM * 2, activation='relu')(model)\n",
    "\n",
    "model  = Dense(units=len(bio_tag2index), activation='relu')(model)\n",
    "\n",
    "    \n",
    "\n",
    "model  = Masking(mask_value=0.,input_shape=(MAX_LENGTH, len(bio_tag2index)))(model)\n",
    "\n",
    "    \n",
    "\n",
    "crf = crf6(units=len(bio_tag2index), name=\"ner_crf\")\n",
    "\n",
    "predictions = crf(model)\n",
    "\n",
    "\n",
    "\n",
    "base_model = Model(inputs=input, outputs=predictions)\n",
    "\n",
    "model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
    "\n",
    "    \n",
    "\n",
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "153/153 - 592s - loss: 20.0810 - accuracy: 0.9783 - val_loss_val: 86.3857 - val_val_accuracy: 0.9217 - 592s/epoch - 4s/step\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "#biobert\n",
    "history= model.fit(bio_train_sentences_X, bio_cat_train_tags_y,\n",
    "\n",
    "                       validation_data=(bio_eval_sentences_X, bio_cat_eval_tags_y),\n",
    "\n",
    "                       batch_size=64, \n",
    "\n",
    "                       epochs=20,\n",
    "\n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 2, 'B-MISC': 3, 'I-ORG': 4, 'B-LOC': 5, 'B-ORG': 6, 'I-PER': 7, 'B-PER': 8, 'O': 9, 'I-MISC': 10, '-PAD-': 0, '-OOV-': 1}\n",
      "48/48 [==============================] - 37s 769ms/step\n",
      "(1517, 202)\n"
     ]
    }
   ],
   "source": [
    "print(tag2index)\n",
    "\n",
    "y_pred= model.predict(test_sentences_X)\n",
    "\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'I-LOC', 3: 'B-MISC', 4: 'I-ORG', 5: 'B-LOC', 6: 'B-ORG', 7: 'I-PER', 8: 'B-PER', 9: 'O', 10: 'I-MISC', 0: '-PAD-', 1: '-OOV-'}\n",
      "['O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "index2tag = {i: t for t, i in tag2index.items()}\n",
    "\n",
    "print(index2tag)\n",
    "\n",
    "y1_pred = logits_to_tokens(y_pred, index2tag)\n",
    "\n",
    "print(y1_pred[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1517, 202)\n"
     ]
    }
   ],
   "source": [
    "#print(Y_test[4])\n",
    "\n",
    "print(test_tags_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'I-LOC', 3: 'B-MISC', 4: 'I-ORG', 5: 'B-LOC', 6: 'B-ORG', 7: 'I-PER', 8: 'B-PER', 9: 'O', 10: 'I-MISC', 0: '-PAD-', 1: '-OOV-'}\n",
      "['O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "\n",
    "index2tag = {i: t for t, i in tag2index.items()}\n",
    "\n",
    "print(index2tag)\n",
    "\n",
    "y1_true = logits_to_tokens(test_tags_y, index2tag)\n",
    "\n",
    "print(y1_true[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.584656Z",
     "iopub.status.idle": "2024-11-19T00:32:33.585021Z",
     "shell.execute_reply": "2024-11-19T00:32:33.584866Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.584849Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 75.3%\n",
      "   recall: 28.0%\n",
      " accuracy: 98.4%\n",
      " F1-score: 40.9%\n"
     ]
    }
   ],
   "source": [
    "#hh1 = seqclarep(results['Expected'], results['Predicted'])\n",
    "\n",
    "#print('\\nclassification_report:\\n', hh1)\n",
    "\n",
    "from seqeval.metrics import classification_report as seqclarep\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(\"precision: {:.1%}\".format(precision_score(y1_true, y1_pred)))\n",
    "\n",
    "print(\"   recall: {:.1%}\".format(recall_score(y1_true,    y1_pred)))\n",
    "\n",
    "print(\" accuracy: {:.1%}\".format(accuracy_score(y1_true,  y1_pred)))\n",
    "\n",
    "print(\" F1-score: {:.1%}\".format(f1_score(y1_true,        y1_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.588026Z",
     "iopub.status.idle": "2024-11-19T00:32:33.588657Z",
     "shell.execute_reply": "2024-11-19T00:32:33.588487Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.588466Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "li1 = sum(y1_true, [])\n",
    "\n",
    "li2 = sum(y1_pred, [])\n",
    "\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['Expected', 'Predicted'])\n",
    "\n",
    "\n",
    "\n",
    "results['Expected'] = li1\n",
    "\n",
    "results['Predicted'] = li2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.590511Z",
     "iopub.status.idle": "2024-11-19T00:32:33.590865Z",
     "shell.execute_reply": "2024-11-19T00:32:33.590710Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.590693Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report as eskclarep\n",
    "\n",
    "report = eskclarep(results['Expected'], results['Predicted'])\n",
    "\n",
    "#print('\\nclassification_report:\\n', report)\n",
    "\n",
    "\n",
    "\n",
    "print(report_to_df(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.592610Z",
     "iopub.status.idle": "2024-11-19T00:32:33.593002Z",
     "shell.execute_reply": "2024-11-19T00:32:33.592838Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.592818Z"
    }
   },
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "\n",
    "    \"James Rodriguez es el jugador colombiano más importante con Radamel Falcao.\".split(),\n",
    "\n",
    "    \" Jugadores de la selección Colombia que juegan en el Reino Unido\".split()\n",
    "\n",
    "]\n",
    "\n",
    "#print(max(test_samples))\n",
    "\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.594672Z",
     "iopub.status.idle": "2024-11-19T00:32:33.595082Z",
     "shell.execute_reply": "2024-11-19T00:32:33.594900Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.594880Z"
    }
   },
   "outputs": [],
   "source": [
    "test_samples_X = []\n",
    "\n",
    "for s in test_samples:\n",
    "\n",
    "    s_int = []\n",
    "\n",
    "    for w in s:\n",
    "\n",
    "        try:\n",
    "\n",
    "            s_int.append(word2index[w.lower()])\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "\n",
    "    test_samples_X.append(s_int)\n",
    "\n",
    "\n",
    "\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(test_samples_X)\n",
    "\n",
    "print(test_samples_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.597357Z",
     "iopub.status.idle": "2024-11-19T00:32:33.598011Z",
     "shell.execute_reply": "2024-11-19T00:32:33.597635Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.597613Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.600028Z",
     "iopub.status.idle": "2024-11-19T00:32:33.600703Z",
     "shell.execute_reply": "2024-11-19T00:32:33.600419Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.600385Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(len(predictions))\n",
    "\n",
    "log_tokens = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print(log_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-19T00:32:33.602493Z",
     "iopub.status.idle": "2024-11-19T00:32:33.603151Z",
     "shell.execute_reply": "2024-11-19T00:32:33.602830Z",
     "shell.execute_reply.started": "2024-11-19T00:32:33.602800Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install tabulate\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "\n",
    "heads1 = test_samples[0]\n",
    "\n",
    "body1 = [log_tokens[0][:len(test_samples[0])]]\n",
    "\n",
    "\n",
    "\n",
    "heads2 = test_samples[1]\n",
    "\n",
    "body2 = [log_tokens[1][:len(test_samples[1])]]\n",
    "\n",
    "\n",
    "\n",
    "print(tabulate(body1, headers=heads1))\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(tabulate(body2, headers=heads2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## postagging Freeling 4.1\n",
    "\n",
    "\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con  bajo  índice   de  adrenalina  .\n",
    "\n",
    "## DA0MS0  NCMS000  AQ0MS00  VMIP3S0  SP    DA0MS0  NCMS000  SP   SP    NCMS000  SP  NCFS000     Fp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## pos tagger Stanford NLP\n",
    "\n",
    "\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con    bajo   índice  de    adrenalina  .\n",
    "\n",
    "## da0000  nc0s000  aq0000   vmip000  sp000 da0000  nc0s000  sp000  aq0000 nc0s000 sp000 nc0s000     fp"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1529457,
     "sourceId": 2524269,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1530256,
     "sourceId": 2525700,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1937937,
     "sourceId": 6353546,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
